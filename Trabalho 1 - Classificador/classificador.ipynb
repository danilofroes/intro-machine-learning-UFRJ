{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e1ed1f",
   "metadata": {},
   "source": [
    "# **Trabalho 1 - Introdução ao Aprendizado de Máquina (EEL891)**\n",
    "### Classificação: sistema de apoio à decisão p/ aprovação de crédito\n",
    "\n",
    "---\n",
    "\n",
    "**Aluno:** Danilo Davi Gomes Fróes\n",
    "\n",
    "**DRE:** 124026825\n",
    "\n",
    "**Usuário Kaggle:** *dgfroes*\n",
    "\n",
    "**Semestre:** 2025.01\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Introdução**\n",
    "\n",
    "O objetivo principal é construir um modelo classificador que, a partir dos dados de um solicitante, seja capaz de predizer com melhor acurácia a probabilidade de ele se tornar inadimplente (variável-alvo `inadimplente`). A finalidade prática de tal modelo é apoiar a decisão de aprovação de crédito, minimizando o risco para a instituição financeira.\n",
    "\n",
    "O trabalho foi desenvolvido usando conhecimentos aprendidos durante as aulas e também do meu projeto de iniciação científica no Laboratório de Simulação e Otimização de Sistemas Produtivos (LASOS) do Instituto Nacional de Tecnologia (INT). O projeto é para o desenvolvimento de uma biblioteca em python de machine learning para otimizar a produção de múltiplas pipelines e gerar insights úteis de retorno para o usuário sem a necessidade de construção de grandes códigos.\n",
    "Deixo aqui um resumo recente do trabalho desenvolvido até então, que futuramente será lançada publicamente para uso da comunidade do python.\n",
    "\n",
    "[Resumo - LasosLibrary](https://docs.google.com/document/d/1T4g_4vvwjcM2SF-lBHUC7RIizCHBxomc/edit?usp=sharing&ouid=107178103419237221884&rtpof=true&sd=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd083df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de Manipulação de Dados e Visualização\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pré-processamento e Validação\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import optuna\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# Remoção de avisos para evitar poluição visual\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de Visualização\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e99964",
   "metadata": {},
   "source": [
    "### **2. Análise Exploratória de Dados (EDA - Exploratory Data Analysis)**\n",
    "Uma parte essencial para treinar modelos de machine learning é entender o que são os dados que estamos utilizando, dessa forma, fiz uma abordagem utilizando funções da biblioteca pandas e o dicionário de dados disponibilizado para obter informações essenciais para o desenvolvimento do projeto.\n",
    "\n",
    "- Nas células abaixo, realizo a obtenção dos dados em CSV como `DataFrames` do pandas e utilizo as funções `info`, `describe` e `value_counts` para uma análise mais aprofundada de cada feature e do balanceamento da target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c89e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino_raw = pd.read_csv('conjunto_de_treinamento.csv')\n",
    "df_teste_raw = pd.read_csv('conjunto_de_teste.csv')\n",
    "\n",
    "print(f\"Info treinamento: {df_treino_raw.info()}\")\n",
    "display(df_treino_raw.describe())\n",
    "print(f\"Target treinamento: {df_treino_raw['inadimplente'].value_counts()}\")\n",
    "\n",
    "print(f\"Info teste: {df_teste_raw.info()}\")\n",
    "display(df_teste_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb72906",
   "metadata": {},
   "source": [
    "### **3. Preparação Inicial e Limpeza dos Dados**\n",
    "\n",
    "A primeira etapa consistiu em estruturar e limpar os dados brutos. O objetivo aqui foi criar uma base de trabalho limpa e consistente. Foram testados diversos tipos de novas features ou manipulação das que existem, porém prejudicava o modelo.\n",
    "\n",
    "#### 3.1. Definição das Matrizes de Features (X) e Vetor Alvo (y)\n",
    "\n",
    "Primeiramente, eu separei a variável-alvo, `inadimplente`, do restante do conjunto de treino, criando o vetor `y`. As demais colunas formaram a matriz de features inicial, `X`.\n",
    "\n",
    "Eu também realizei uma remoção seletiva de colunas com base no dicionário de dados e em uma análise inicial:\n",
    "* `id_solicitante`: Removido por ser um identificador único, sem poder preditivo.\n",
    "* `inadimplente`: Removido da matriz `X` por ser a variável-alvo.\n",
    "* `possui_telefone_celular`: Removido pois, conforme o dicionário de dados, esta coluna possuía variância nula (todos os valores eram \"N\"), não contendo informação útil para o modelo.\n",
    "\n",
    "Algumas colunas foram mantidas, mesmo que recomendadas a serem retiradas, pois mudavam o desempenho.\n",
    "\n",
    "#### 3.2. Tratamento de Valores Ausentes Ocultos\n",
    "\n",
    "Durante a inspeção dos dados, notei que muitos valores ausentes não estavam no formato padrão `NaN` (Not a Number), mas sim representados por strings como `''`, `'XX'` ou `' '`. Estes são \"nulos ocultos\" que não seriam detectados automaticamente pelas ferramentas de imputação.\n",
    "\n",
    "Para corrigir isso, eu substituí todas as ocorrências dessas strings por `np.nan`. Essa padronização é um passo de limpeza fundamental, garantindo que todo e qualquer dado faltante seja corretamente reconhecido e tratado na etapa de pré-processamento do pipeline.\n",
    "\n",
    "#### 3.3. Alinhamento dos Conjuntos de Treino e Teste\n",
    "\n",
    "Para garantir a consistência e evitar erros durante a predição, é essencial que os dataframes de treino e teste tenham exatamente a mesma estrutura de colunas. A linha `X_teste = X_teste[X.columns]` assegura esse alinhamento, reordenando as colunas do conjunto de teste para que correspondam perfeitamente às do conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar IDs e alvo\n",
    "ids_teste = df_teste_raw['id_solicitante']\n",
    "y = df_treino_raw['inadimplente']\n",
    "\n",
    "# Remover colunas com baixa variância ou redundantes\n",
    "colunas_a_remover = ['id_solicitante', 'inadimplente', 'possui_telefone_celular']\n",
    "X = df_treino_raw.drop(columns=colunas_a_remover, errors='ignore')\n",
    "X_teste = df_teste_raw.drop(columns=colunas_a_remover, errors='ignore')\n",
    "\n",
    "# Limpeza de nulos ocultos\n",
    "nulos_ocultos = [' ', 'N/A', '', '?', 'XX', 'NULL']\n",
    "X.replace(nulos_ocultos, np.nan, inplace=True)\n",
    "X_teste.replace(nulos_ocultos, np.nan, inplace=True)\n",
    "\n",
    "# Garantir consistência das colunas\n",
    "X_teste = X_teste[X.columns]\n",
    "\n",
    "print(f\"Número de features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408ee06",
   "metadata": {},
   "source": [
    "### **4. Pipeline de Pré-processamento e Estratégia de Validação**\n",
    "\n",
    "Para garantir que o processo de transformação dos dados fosse robusto, reprodutível e livre de vazamento de dados (*data leakage*), eu construí um pipeline de pré-processing unificado utilizando o `ColumnTransformer` do Scikit-learn. Em paralelo, defini uma estratégia de validação cruzada para assegurar uma avaliação fidedigna da performance dos modelos.\n",
    "\n",
    "#### 4.1. Tratamento de Features Numéricas\n",
    "\n",
    "Para as colunas numéricas, eu criei um pipeline com dois passos essenciais:\n",
    "\n",
    "1.  **Imputação com Mediana (`SimpleImputer`):** Para tratar os valores ausentes (`NaN`), optei por preenchê-los com a **mediana** da respectiva coluna. A mediana é uma medida de tendência central mais robusta a outliers do que a média, sendo a escolha ideal para dados financeiros e demográficos, que frequentemente possuem distribuições assimétricas.\n",
    "2.  **Escalonamento Padrão (`StandardScaler`):** Após a imputação, eu padronizei as features numéricas. O `StandardScaler` transforma os dados para que tenham média (μ) igual a 0 e desvio padrão (σ) igual a 1. Este passo é crucial para o bom desempenho de muitos algoritmos, incluindo a Regressão Logística (que usei como meta-modelo no ensemble), pois garante que nenhuma feature domine o processo de aprendizado apenas por ter uma escala de magnitude maior que as outras.\n",
    "\n",
    "#### 4.2. Tratamento de Features Categóricas\n",
    "\n",
    "Para as colunas categóricas (que contêm texto ou códigos), o pipeline também seguiu dois passos:\n",
    "\n",
    "1.  **Imputação com Moda (`SimpleImputer`):** Os valores ausentes foram preenchidos com a categoria mais frequente (\"moda\") da coluna. Esta é a abordagem padrão e mais lógica para dados categóricos.\n",
    "2.  **Encoding com `OneHotEncoder`:** Como os modelos de machine learning requerem input numérico, eu converti as categorias em um formato que eles pudessem entender. Optei pelo **One-Hot Encoding**, que cria uma nova coluna binária (0 ou 1) para cada categoria única. Escolhi este método para evitar a criação de uma falsa relação ordinal que outros encoders (como o `LabelEncoder`) poderiam introduzir. O parâmetro `handle_unknown='ignore'` foi fundamental para que o modelo lidasse de forma elegante com categorias que pudessem aparecer no conjunto de teste sem terem sido vistas no treino.\n",
    "\n",
    "#### 4.3. Estratégia de Validação Cruzada\n",
    "\n",
    "Para avaliar os modelos, eu utilizei a **Validação Cruzada Estratificada (`StratifiedKFold`)** com 10 folds. A estratificação garante que a proporção de inadimplentes e bons pagadores (as classes da variável-alvo) seja a mesma em cada um dos 10 folds, espelhando a distribuição do dataset original. Isso é essencial em problemas de classificação para evitar que um fold tenha, por acaso, uma concentração muito diferente das classes, o que levaria a uma estimativa de performance instável e pouco confiável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78059ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica colunas numéricas e categóricas\n",
    "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Pipeline para features numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para features categóricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combina os pipelines no ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define a estratégia de validação cruzada\n",
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8accc",
   "metadata": {},
   "source": [
    "### **5. Otimização de Hiperparâmetros**\n",
    "\n",
    "Com a estrutura de dados e o pipeline de pré-processamento definidos, o passo seguinte foi focar em extrair o máximo de performance dos algoritmos de machine learning. Para isso, realizei um processo de **otimização de hiperparâmetros**, que consiste em encontrar a melhor combinação de configurações para cada modelo.\n",
    "\n",
    "#### 5.1. A Escolha da Estratégia: Otimização Bayesiana com Optuna\n",
    "\n",
    "Para realizar essa busca, eu optei por utilizar a biblioteca `Optuna`, que implementa a **Otimização Bayesiana**. Escolhi esta abordagem em detrimento de métodos mais simples como *Grid Search* (ineficiente para espaços de busca grandes) ou *Random Search* (eficiente, mas não-inteligente) pelas seguintes razões:\n",
    "\n",
    "A Otimização Bayesiana é um método de busca sequencial e inteligente. Ela trata a otimização como um problema de probabilidade, construindo um modelo estatístico interno (surrogate model) que mapeia os hiperparâmetros aos scores de performance. A cada iteração, ela utiliza os resultados das tentativas anteriores para decidir qual a próxima combinação de parâmetros tem a maior probabilidade de gerar um resultado melhor. Esse processo equilibra de forma eficiente a **exploração** (testar novas áreas do espaço de busca) e a **explotação** (focar em áreas que já se mostraram promissoras), permitindo encontrar soluções de alta qualidade com um número menor de tentativas.\n",
    "\n",
    "#### 5.2. O Processo de Otimização\n",
    "\n",
    "Minha implementação foi estruturada em torno de uma função `objective` genérica, projetada para avaliar qualquer um dos modelos candidatos. Para cada modelo que eu desejava otimizar (`LGBMClassifier` e `XGBClassifier`), um \"estudo\" do Optuna foi executado:\n",
    "\n",
    "1.  **Espaço de Busca:** Para cada modelo, eu defini um espaço de busca customizado, focando nos seus hiperparâmetros mais influentes, como `n_estimators`, `learning_rate`, `max_depth`, e parâmetros de regularização (`reg_alpha`, `reg_lambda`, `gamma`), que são essenciais para controlar o *overfitting*.\n",
    "\n",
    "2.  **Avaliação:** Em cada uma das 100 tentativas (`n_trials`), a função `objective` construiu um pipeline completo (pré-processador + modelo com os parâmetros da tentativa) e o avaliou utilizando nossa robusta estratégia de validação cruzada estratificada (`cv_strategy`).\n",
    "\n",
    "3.  **Resultado:** A acurácia média dos 10 folds foi retornada ao `study` do Optuna, que utilizou essa informação para refinar sua busca. Ao final do processo, os melhores parâmetros encontrados para cada modelo foram armazenados para serem utilizados na construção do nosso ensemble final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9442b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para guardar os melhores parâmetros\n",
    "best_params_dict = {}\n",
    "\n",
    "def objective(trial, model_class):\n",
    "    if model_class == LGBMClassifier:\n",
    "        params = {\n",
    "            'random_state': 42, 'n_jobs': -1, 'verbose': -1,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        }\n",
    "    elif model_class == XGBClassifier:\n",
    "        params = {\n",
    "            'random_state': 42, 'n_jobs': -1, 'eval_metric': 'logloss',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_int('gamma', 0, 5)\n",
    "        }\n",
    "    \n",
    "    model = model_class(**params)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    score = cross_val_score(pipeline, X, y, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
    "    return score.mean()\n",
    "\n",
    "# Otimizando os modelos\n",
    "for name, model_class in [(\"LGBM\", LGBMClassifier), (\"XGB\", XGBClassifier)]:\n",
    "    print(f\"\\nOtimizando {name}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, model_class), n_trials=100)\n",
    "    best_params_dict[name] = study.best_params\n",
    "    print(f\"Melhor Acurácia para {name}: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe0639",
   "metadata": {},
   "source": [
    "### **6. Modelo Final: Ensemble com StackingClassifier**\n",
    "\n",
    "Após o processo de otimização, eu possuía duas versões altamente ajustadas dos modelos mais promissores: LightGBM e XGBoost. A etapa final consistiu em combinar a força preditiva de ambos em um único modelo superior, utilizando uma técnica de ensemble avançada conhecida como **Stacking** (ou empilhamento).\n",
    "\n",
    "#### 6.1. A Estratégia de Ensemble: Stacking\n",
    "\n",
    "Eu optei pelo Stacking por ser um método que, em vez de uma simples votação ou média, treina um **meta-modelo** para aprender a melhor forma de combinar as previsões dos modelos base. A intuição é criar um \"comitê de especialistas\" (os modelos base) e um \"diretor\" (o meta-modelo) que aprende a ponderar a opinião de cada especialista para tomar a decisão final mais acurada.\n",
    "\n",
    "O processo, gerenciado pelo `StackingClassifier`, funciona da seguinte forma:\n",
    "1.  As previsões dos modelos base (LGBM e XGB) são geradas através da nossa estratégia de validação cruzada (`StratifiedKFold`).\n",
    "2.  Essas previsões \"fora da amostra\" (*Out-of-Fold*) são então usadas como features para treinar o meta-modelo. Eu utilizei o método `predict_proba`, que alimenta o meta-modelo com as probabilidades previstas, um sinal muito mais rico e informativo do que a classe final (0 ou 1).\n",
    "3.  O meta-modelo aprende a mapear essas probabilidades de entrada para a previsão final.\n",
    "\n",
    "#### 6.2. Componentes do Modelo de Stacking\n",
    "\n",
    "* **Modelos Base:** Utilizei as duas versões dos modelos de gradient boosting, com seus hiperparâmetros otimizados pela busca Bayesiana do Optuna. Para maximizar a **diversidade** do ensemble, eu configurei cada modelo com um `random_state` diferente, sendo ambos valores primos e da sequência de Fibonacci, garantindo que a aleatoriedade interna de cada um gerasse modelos com \"perspectivas\" ligeiramente distintas.\n",
    "\n",
    "* **Meta-Modelo (`final_estimator`):** Para o meta-modelo, eu escolhi a `LogisticRegression`. Utilizar um modelo linear simples e robusto nesta etapa é uma prática recomendada, pois ele aprende a combinação linear ótima das previsões dos modelos base sem o risco de se sobreajustar a elas, o que melhora a capacidade de generalização do ensemble final.\n",
    "\n",
    "#### 6.3. Treinamento e Geração da Submissão\n",
    "\n",
    "O `StackingClassifier` foi integrado a um `Pipeline` final junto com o `preprocessor`. Este pipeline foi treinado com todos os dados de treino disponíveis, garantindo o máximo aprendizado. Por fim, o pipeline treinado foi utilizado para gerar as previsões no conjunto de teste, resultando no arquivo de submissão final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie instâncias dos modelos com os melhores parâmetros\n",
    "lgbm_final = LGBMClassifier(**best_params_dict['LGBM'], random_state=13)\n",
    "xgb_final = XGBClassifier(**best_params_dict['XGB'], random_state=55)\n",
    "\n",
    "# O meta-modelo pode ser uma Regressão Logística, que é simples e robusta\n",
    "meta_model = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Defina a lista de estimadores para o Stacking\n",
    "estimators = [\n",
    "    ('LGBM', lgbm_final),\n",
    "    ('XGBoost', xgb_final)\n",
    "]\n",
    "\n",
    "# Crie o Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_strategy,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Crie o pipeline final\n",
    "stacking_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stacker', stacking_model)\n",
    "])\n",
    "\n",
    "colunas_para_adicionar = ['grau_instrucao', 'qtde_contas_bancarias_especiais']\n",
    "\n",
    "# Adiciona as colunas do dataframe original para os dataframes X e X_teste\n",
    "X[colunas_para_adicionar] = df_treino_raw[colunas_para_adicionar]\n",
    "X_teste[colunas_para_adicionar] = df_teste_raw[colunas_para_adicionar]\n",
    "\n",
    "# Treine o modelo final com todos os dados de treino\n",
    "print(\"\\nTreinando o modelo de Stacking final...\")\n",
    "stacking_pipeline.fit(X, y)\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "# Faça as predições\n",
    "final_predictions = stacking_pipeline.predict(X_teste)\n",
    "\n",
    "# Crie e salve o arquivo de submissão\n",
    "submission_df = pd.DataFrame({'id_solicitante': ids_teste, 'inadimplente': final_predictions})\n",
    "submission_df.to_csv('submission_FINAL_classificacao.csv', index=False)\n",
    "\n",
    "print(\"\\nArquivo 'submission_FINAL_classificacao.csv' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a1e24",
   "metadata": {},
   "source": [
    "### **7. Conclusão e Resultados**\n",
    "\n",
    "Este projeto abordou de forma completa o desafio de classificação de risco de crédito, desde a limpeza inicial dos dados até a implementação de um modelo de ensemble avançado. O processo foi iterativo e guiado por uma estratégia de validação robusta, permitindo a construção de uma solução de alta performance.\n",
    "\n",
    "O resultado final foi um `StackingClassifier` que combinou as forças de dois modelos de gradient boosting (LightGBM e XGBoost), otimizados individualmente.\n",
    "\n",
    "Os principais aprendizados e fatores de sucesso foram:\n",
    "* A importância de uma **validação cruzada estratificada** para obter estimativas de performance confiáveis.\n",
    "* A eficiência da **Otimização Bayesiana (Optuna)** para explorar espaços de busca complexos e encontrar hiperparâmetros de alta qualidade.\n",
    "* O poder do **Stacking** como técnica de ensemble, que se provou superior aos modelos individuais ao aprender a melhor forma de combinar suas previsões."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
